/*
 *  {{ neuronName }}.cu
 *
 *  This file is part of NEST GPU.
 *
 *  Copyright (C) 2021 The NEST Initiative
 *
 *  NEST GPU is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation, either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  NEST GPU is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with NEST GPU.  If not, see <http://www.gnu.org/licenses/>.
 *
 */

#include <config.h>
#include <cmath>
#include <iostream>
#include "{{ neuronName }}.h"
#include "spike_buffer.h"

using namespace {{ neuronName }}_ns;

extern __constant__ float NESTGPUTimeResolution;

{%- for variable_symbol in neuron.get_state_symbols() %}
{%-     set variable = utils.get_state_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
#define {{ printer_no_origin.print(variable) }} var[i_{{ printer_no_origin.print(variable) }}]
{%- endfor %}

{%- for variable_symbol in neuron.get_parameter_symbols() %}
{%-     set variable = utils.get_parameter_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
#define {{ printer_no_origin.print(variable) }} param[i_{{ printer_no_origin.print(variable) }}]
{%- endfor %}

{%- for variable_symbol in neuron.get_internal_symbols() %}
{%-     set variable = utils.get_internal_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
#define {{ printer_no_origin.print(variable) }} param[i_{{ printer_no_origin.print(variable) }}]
{%- endfor %}

__global__ void {{ neuronName }}_Calibrate(int n_node, float *param_arr,
				      int n_param, float __h)
{
  int i_neuron = threadIdx.x + blockIdx.x * blockDim.x;
  if (i_neuron < n_node) {
    float *param = param_arr + n_param*i_neuron;

{%- filter indent(4,True) %}
{%- for internals_block in neuron.get_internals_blocks() %}
{%-     for decl in internals_block.get_declarations() %}
{%-         for variable in decl.get_variables() %}
{%-             include "directives/MemberInitialization.jinja2" %}
{%-         endfor %}
{%-     endfor %}
{%- endfor %}
{%- endfilter %}
  }
}


__global__ void {{ neuronName }}_Update(int n_node, int i_node_0, float *var_arr,
				   float *param_arr, int n_var, int n_param)
{
  int i_neuron = threadIdx.x + blockIdx.x * blockDim.x;
  if (i_neuron < n_node) {
    float *var = var_arr + n_var*i_neuron;
    float *param = param_arr + n_param*i_neuron;

{%- if neuron.get_update_blocks() %}
{%-     filter indent(2) %}
{%-         for block in neuron.get_update_blocks() %}
{%-             set ast = block.get_block() %}
{%-             if ast.print_comment('*')|length > 1 %}
/*
 {{ast.print_comment('*')}}
 */
{%-             endif %}
{%-             include "directives/Block.jinja2" %}
{%-         endfor %}
{%-     endfilter %}
{%- endif %}
  }
}

{{ neuronName }}::~{{ neuronName }}()
{
  FreeVarArr();
  FreeParamArr();
}

int {{ neuronName }}::Init(int i_node_0, int n_node, int /*n_port*/,
			 int i_group, unsigned long long *seed)
{
  BaseNeuron::Init(i_node_0, n_node, 2 /*n_port*/, i_group, seed);
  node_type_ = i_{{ neuronName }}_model;

  n_scal_var_ = N_SCAL_VAR;
  n_var_ = n_scal_var_;
  n_scal_param_ = N_SCAL_PARAM;
  n_param_ = n_scal_param_;

  AllocParamArr();
  AllocVarArr();

  scal_var_name_ = {{ neuronName }}_scal_var_name;
  scal_param_name_ = {{ neuronName }}_scal_param_name;

{%- for variable_symbol in neuron.get_parameter_symbols() %}
{%-     set variable = utils.get_parameter_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
    SetScalParam(0, n_node, "{{ printer_no_origin.print(variable) }}", {{printer.print(variable_symbol.get_declaring_expression())}});  // as {{variable_symbol.get_type_symbol().print_symbol()}}
{%- endfor %}

{%- for variable_symbol in neuron.get_internal_symbols() %}
{%-     set variable = utils.get_internal_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
    SetScalParam(0, n_node, "{{ printer_no_origin.print(variable) }}", 0.0);
{%- endfor %}

{%- for variable_symbol in neuron.get_state_symbols() %}
{%-     set variable = utils.get_state_variable_by_name(astnode, variable_symbol.get_symbol_name()) %}
    SetScalVar(0, n_node, "{{ printer_no_origin.print(variable) }}", {{printer.print(variable_symbol.get_declaring_expression())}});
{%- endfor %}

  // multiplication factor of input signal is always 1 for all nodes
  float input_weight = 1.0;
  gpuErrchk(cudaMalloc(&port_weight_arr_, sizeof(float)));
  gpuErrchk(cudaMemcpy(port_weight_arr_, &input_weight,
			 sizeof(float), cudaMemcpyHostToDevice));
  port_weight_arr_step_ = 0;
  port_weight_port_step_ = 0;

  // input spike signal is stored in I_syn_ex, I_syn_in
  port_input_arr_ = GetVarArr() + GetScalVarIdx("I_kernel_exc__X__exc_spikes");
  port_input_arr_step_ = n_var_;
  port_input_port_step_ = 1;

{#  den_delay_arr_ =  GetParamArr() + GetScalParamIdx("den_delay");#}

  return 0;
}

int {{ neuronName }}::Update(long long it, double t1)
{
  {{ neuronName }}_Update<<<(n_node_+1023)/1024, 1024>>>
    (n_node_, i_node_0_, var_arr_, param_arr_, n_var_, n_param_);
  // gpuErrchk( cudaDeviceSynchronize() );

  return 0;
}

int {{ neuronName }}::Free()
{
  FreeVarArr();
  FreeParamArr();

  return 0;
}

int {{ neuronName }}::Calibrate(double, float time_resolution)
{
  {{ neuronName }}_Calibrate<<<(n_node_+1023)/1024, 1024>>>
    (n_node_, param_arr_, n_param_, time_resolution);

  return 0;
}

